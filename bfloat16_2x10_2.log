18:22:22.775 774863 POPRUN [I] V-IPU server address picked up from 'vipu': 10.1.3.101:8090
18:22:22.778 774863 POPRUN [I] Using V-IPU partition p64 as it is the only one available
18:22:22.778 774863 POPRUN [D] Connecting to 10.1.3.101:8090
18:22:22.781 774863 POPRUN [D] Status for partition p64: OK (error 0)
18:22:22.781 774863 POPRUN [I] Partition p64 already exists and is in state: PS_ACTIVE
18:22:22.782 774863 POPRUN [D] The reconfigurable partition p64 is OK
 ===================
|  poprun topology  |
|===================|
| hosts     |  [0]  |
|-----------|-------|
| ILDs      |   0   |
|-----------|-------|
| instances | 0 | 1 |
|-----------|-------|
| replicas  | 0 | 1 |
 -------------------
[0]: localhost
18:22:22.784 774863 POPRUN [D] Target options from environment: {}
18:22:22.784 774863 POPRUN [D] Target options from V-IPU partition: {"ipuLinkDomainSize":"64","ipuLinkConfiguration":"slidingWindow","ipuLinkTopology":"mesh","gatewayMode":"true","instanceSize":"32"}
18:22:23.312 774863 POPRUN [D] Found 32 devices with 2 IPUs
2023-02-03T18:22:23.729369Z ipuof_lib 774863.774899 E: attach_device_impl: Device already in use, device 3, IP 10.1.5.1:50052
2023-02-03T18:22:23.881995Z PO:TARGET   774863.774863 E: Failed to attach to ipu(s)
18:22:23.882 774863 POPRUN [D] Failed to attach to device 64
18:22:24.678 774863 POPRUN [D] Attached to device 65
18:22:24.678 774863 POPRUN [D] Found 2 children (expected 2) of device 65 with 1 IPUs each
18:22:24.679 774863 POPRUN [I] Preparing parent device 65
18:22:24.679 774863 POPRUN [D] Device 65 ipuLinkDomainSize=64, ipuLinkConfiguration=Default, ipuLinkTopology=Mesh, gatewayMode=true, instanceSize=1
18:22:26.943 774863 POPRUN [D] Target options from Poplar device: {"ipuLinkDomainSize":"64","ipuLinkConfiguration":"default","ipuLinkTopology":"mesh","gatewayMode":"true","instanceSize":"1"}
18:22:26.943 774863 POPRUN [D] Using target options: {"ipuLinkDomainSize":"64","ipuLinkConfiguration":"slidingWindow","ipuLinkTopology":"mesh","gatewayMode":"true","instanceSize":"32"}
18:22:26.970 774863 POPRUN [D] Skipping port 1 of IB device uverbs0 because its state (1) doesn't seem active. Information in sysfs might be invalid or might not exist
18:22:26.976 774863 POPRUN [D] Skipping port 1 of IB device uverbs1 because its state (1) doesn't seem active. Information in sysfs might be invalid or might not exist
18:22:26.990 774863 POPRUN [D] No hosts specified; ignoring host-subnet setting
18:22:26.990 774863 POPRUN [D] Default network/RNIC for host communication: None
18:22:26.991 774863 POPRUN [I] Running command: /software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/bin/mpirun '--bind-to' 'none' '--tag-output' '--allow-run-as-root' '-np' '1' '-x' 'POPDIST_NUM_TOTAL_REPLICAS=2' '-x' 'POPDIST_NUM_IPUS_PER_REPLICA=1' '-x' 'POPDIST_NUM_LOCAL_REPLICAS=1' '-x' 'POPDIST_UNIFORM_REPLICAS_PER_INSTANCE=1' '-x' 'POPDIST_NODE_DEVICE_ID=65' '-x' 'POPDIST_EXECUTABLE_CACHE_PATH=/home/wilsonb/tmp' '-x' 'POPDIST_REPLICA_INDEX_OFFSET=0' '-x' 'POPDIST_LOCAL_INSTANCE_INDEX=0' '-x' 'IPUOF_VIPU_API_HOST=10.1.3.101' '-x' 'IPUOF_VIPU_API_PORT=8090' '-x' 'IPUOF_VIPU_API_PARTITION_ID=p64' '-x' 'IPUOF_VIPU_API_TIMEOUT=120' '-x' 'IPUOF_LOG_LEVEL=WARN' '-x' 'PATH' '-x' 'LD_LIBRARY_PATH' '-x' 'PYTHONPATH' '-x' 'POPLAR_TARGET_OPTIONS={"ipuLinkDomainSize":"64","ipuLinkConfiguration":"slidingWindow","ipuLinkTopology":"mesh","gatewayMode":"true","instanceSize":"32"}' 'poprun_numa_wrapper' 'python' 'bin/exec.py' '--config-name=a21' 'mode=train' 'run.id=' 'run.distributed=False' 'data=real' 'data.data_directory=/lambda_stor/data/datascience/cosmic_tagging/' 'framework=torch' 'run.compute_mode=IPU' 'run.minibatch_size=2' 'run.iterations=10' 'run.precision=3' ':' '-np' '1' '-x' 'POPDIST_NUM_TOTAL_REPLICAS=2' '-x' 'POPDIST_NUM_IPUS_PER_REPLICA=1' '-x' 'POPDIST_NUM_LOCAL_REPLICAS=1' '-x' 'POPDIST_UNIFORM_REPLICAS_PER_INSTANCE=1' '-x' 'POPDIST_NODE_DEVICE_ID=65' '-x' 'POPDIST_EXECUTABLE_CACHE_PATH=/home/wilsonb/tmp' '-x' 'POPDIST_REPLICA_INDEX_OFFSET=1' '-x' 'POPDIST_LOCAL_INSTANCE_INDEX=1' '-x' 'IPUOF_VIPU_API_HOST=10.1.3.101' '-x' 'IPUOF_VIPU_API_PORT=8090' '-x' 'IPUOF_VIPU_API_PARTITION_ID=p64' '-x' 'IPUOF_VIPU_API_TIMEOUT=120' '-x' 'IPUOF_LOG_LEVEL=WARN' '-x' 'PATH' '-x' 'LD_LIBRARY_PATH' '-x' 'PYTHONPATH' '-x' 'POPLAR_TARGET_OPTIONS={"ipuLinkDomainSize":"64","ipuLinkConfiguration":"slidingWindow","ipuLinkTopology":"mesh","gatewayMode":"true","instanceSize":"32"}' 'poprun_numa_wrapper' 'python' 'bin/exec.py' '--config-name=a21' 'mode=train' 'run.id=' 'run.distributed=False' 'data=real' 'data.data_directory=/lambda_stor/data/datascience/cosmic_tagging/' 'framework=torch' 'run.compute_mode=IPU' 'run.minibatch_size=2' 'run.iterations=10' 'run.precision=3'
18:22:27.002 774863 POPRUN [I] Waiting for mpirun (PID 775437)
[1,0]<stdout>:2023-02-03 18:22:28,086 - INFO - Dumping launch arguments.
[1,0]<stdout>:2023-02-03 18:22:28,086 - INFO - ['bin/exec.py', '--config-name=a21', 'mode=train', 'run.id=', 'run.distributed=False', 'data=real', 'data.data_directory=/lambda_stor/data/datascience/cosmic_tagging/', 'framework=torch', 'run.compute_mode=IPU', 'run.minibatch_size=2', 'run.iterations=10', 'run.precision=3', 'hydra/job_logging=disabled']
[1,0]<stdout>:2023-02-03 18:22:28,086 - INFO - Running Training
[1,1]<stdout>:2023-02-03 18:22:28,089 - INFO - Dumping launch arguments.
[1,1]<stdout>:2023-02-03 18:22:28,089 - INFO - ['bin/exec.py', '--config-name=a21', 'mode=train', 'run.id=', 'run.distributed=False', 'data=real', 'data.data_directory=/lambda_stor/data/datascience/cosmic_tagging/', 'framework=torch', 'run.compute_mode=IPU', 'run.minibatch_size=2', 'run.iterations=10', 'run.precision=3', 'hydra/job_logging=disabled']
[1,1]<stdout>:2023-02-03 18:22:28,089 - INFO - Running Training
[1,0]<stdout>:2023-02-03 18:22:28,093 - INFO - 
[1,0]<stdout>:
[1,0]<stdout>:-- CONFIG --
[1,0]<stdout>:data: 
[1,0]<stdout>:  aux_file....................: cosmic_tagging_val.h5
[1,0]<stdout>:  data_directory..............: /lambda_stor/data/datascience/cosmic_tagging/
[1,0]<stdout>:  data_format.................: channels_first
[1,0]<stdout>:  downsample..................: 1
[1,0]<stdout>:  file........................: cosmic_tagging_train.h5
[1,0]<stdout>:  img_transform...............: False
[1,0]<stdout>:  synthetic...................: False
[1,0]<stdout>:framework: 
[1,0]<stdout>:  distributed_mode............: DDP
[1,0]<stdout>:  name........................: torch
[1,0]<stdout>:  sparse......................: False
[1,0]<stdout>:mode: 
[1,0]<stdout>:  checkpoint_iteration........: 500
[1,0]<stdout>:  logging_iteration...........: 1
[1,0]<stdout>:  name........................: train
[1,0]<stdout>:  no_summary_images...........: True
[1,0]<stdout>:  optimizer: 
[1,0]<stdout>:    gradient_accumulation.....: 1
[1,0]<stdout>:    learning_rate.............: 0.0003
[1,0]<stdout>:    loss_balance_scheme.......: focal
[1,0]<stdout>:    name......................: adam
[1,0]<stdout>:  quantization_aware..........: False
[1,0]<stdout>:  summary_iteration...........: 1
[1,0]<stdout>:  weights_location............: 
[1,0]<stdout>:network: 
[1,0]<stdout>:  bias........................: True
[1,0]<stdout>:  block_concat................: False
[1,0]<stdout>:  blocks_deepest_layer........: 5
[1,0]<stdout>:  blocks_final................: 5
[1,0]<stdout>:  blocks_per_layer............: 2
[1,0]<stdout>:  bottleneck_deepest..........: 256
[1,0]<stdout>:  connections.................: concat
[1,0]<stdout>:  conv_mode...................: conv_2D
[1,0]<stdout>:  data_format.................: channels_first
[1,0]<stdout>:  depth.......................: 6
[1,0]<stdout>:  downsampling................: max_pooling
[1,0]<stdout>:  filter_size_deepest.........: 5
[1,0]<stdout>:  growth_rate.................: additive
[1,0]<stdout>:  n_initial_filters...........: 8
[1,0]<stdout>:  name........................: A21
[1,0]<stdout>:  normalization...............: batch
[1,0]<stdout>:  residual....................: False
[1,0]<stdout>:  upsampling..................: interpolation
[1,0]<stdout>:  weight_decay................: 0.1
[1,0]<stdout>:output_dir....................: output/torch/A21//
[1,0]<stdout>:run: 
[1,0]<stdout>:  aux_iterations..............: 10
[1,0]<stdout>:  compute_mode................: IPU
[1,0]<stdout>:  distributed.................: False
[1,0]<stdout>:  id..........................: 
[1,0]<stdout>:  iterations..................: 10
[1,0]<stdout>:  minibatch_size..............: 2
[1,0]<stdout>:  precision...................: float16
[1,0]<stdout>:  profile.....................: False
[1,0]<stdout>:
[1,1]<stdout>:2023-02-03 18:22:28,096 - INFO - 
[1,1]<stdout>:
[1,1]<stdout>:-- CONFIG --
[1,1]<stdout>:data: 
[1,1]<stdout>:  aux_file....................: cosmic_tagging_val.h5
[1,1]<stdout>:  data_directory..............: /lambda_stor/data/datascience/cosmic_tagging/
[1,1]<stdout>:  data_format.................: channels_first
[1,1]<stdout>:  downsample..................: 1
[1,1]<stdout>:  file........................: cosmic_tagging_train.h5
[1,1]<stdout>:  img_transform...............: False
[1,1]<stdout>:  synthetic...................: False
[1,1]<stdout>:framework: 
[1,1]<stdout>:  distributed_mode............: DDP
[1,1]<stdout>:  name........................: torch
[1,1]<stdout>:  sparse......................: False
[1,1]<stdout>:mode: 
[1,1]<stdout>:  checkpoint_iteration........: 500
[1,1]<stdout>:  logging_iteration...........: 1
[1,1]<stdout>:  name........................: train
[1,1]<stdout>:  no_summary_images...........: True
[1,1]<stdout>:  optimizer: 
[1,1]<stdout>:    gradient_accumulation.....: 1
[1,1]<stdout>:    learning_rate.............: 0.0003
[1,1]<stdout>:    loss_balance_scheme.......: focal
[1,1]<stdout>:    name......................: adam
[1,1]<stdout>:  quantization_aware..........: False
[1,1]<stdout>:  summary_iteration...........: 1
[1,1]<stdout>:  weights_location............: 
[1,1]<stdout>:network: 
[1,1]<stdout>:  bias........................: True
[1,1]<stdout>:  block_concat................: False
[1,1]<stdout>:  blocks_deepest_layer........: 5
[1,1]<stdout>:  blocks_final................: 5
[1,1]<stdout>:  blocks_per_layer............: 2
[1,1]<stdout>:  bottleneck_deepest..........: 256
[1,1]<stdout>:  connections.................: concat
[1,1]<stdout>:  conv_mode...................: conv_2D
[1,1]<stdout>:  data_format.................: channels_first
[1,1]<stdout>:  depth.......................: 6
[1,1]<stdout>:  downsampling................: max_pooling
[1,1]<stdout>:  filter_size_deepest.........: 5
[1,1]<stdout>:  growth_rate.................: additive
[1,1]<stdout>:  n_initial_filters...........: 8
[1,1]<stdout>:  name........................: A21
[1,1]<stdout>:  normalization...............: batch
[1,1]<stdout>:  residual....................: False
[1,1]<stdout>:  upsampling..................: interpolation
[1,1]<stdout>:  weight_decay................: 0.1
[1,1]<stdout>:output_dir....................: output/torch/A21//
[1,1]<stdout>:run: 
[1,1]<stdout>:  aux_iterations..............: 10
[1,1]<stdout>:  compute_mode................: IPU
[1,1]<stdout>:  distributed.................: False
[1,1]<stdout>:  id..........................: 
[1,1]<stdout>:  iterations..................: 10
[1,1]<stdout>:  minibatch_size..............: 2
[1,1]<stdout>:  precision...................: float16
[1,1]<stdout>:  profile.....................: False
[1,1]<stdout>:
[1,0]<stdout>:2023-02-03 18:22:30,841 - INFO - Larcv file prepared
[1,1]<stdout>:2023-02-03 18:22:30,953 - INFO - Larcv file prepared
[1,0]<stdout>:2023-02-03 18:22:31,032 - INFO - Larcv file prepared
[1,1]<stdout>:2023-02-03 18:22:31,113 - INFO - Larcv file prepared
[1,0]<stdout>:2023-02-03 18:22:31,172 - INFO - Total number of trainable parameters in this network: 8525699
[1,0]<stderr>:2023-02-03 18:22:31.194296: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
[1,0]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[1,1]<stdout>:2023-02-03 18:22:31,239 - INFO - Total number of trainable parameters in this network: 8525699
[1,1]<stderr>:2023-02-03 18:22:31.255171: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
[1,1]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
[1,0]<stderr>:2023-02-03 18:22:31.330997: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/popart-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib
[1,0]<stderr>:2023-02-03 18:22:31.331036: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[1,1]<stderr>:2023-02-03 18:22:31.411331: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/popart-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib
[1,1]<stderr>:2023-02-03 18:22:31.411372: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[1,0]<stderr>:2023-02-03 18:22:33.599638: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/popart-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib
[1,0]<stderr>:2023-02-03 18:22:33.599709: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/popart-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib
[1,0]<stderr>:2023-02-03 18:22:33.599715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
[1,1]<stderr>:2023-02-03 18:22:33.600214: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/popart-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib
[1,1]<stderr>:2023-02-03 18:22:33.600275: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/popart-ubuntu_20_04-3.1.0+6824-9c103dc348/lib:/software/graphcore/poplar_sdk/3.1.0/poplar-ubuntu_20_04-3.1.0+6824-9c103dc348/lib
[1,1]<stderr>:2023-02-03 18:22:33.600280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
[1,0]<stdout>:2023-02-03 18:22:37,462 - INFO - No checkpoint file found, restarting from scratch
[1,1]<stdout>:2023-02-03 18:22:37,467 - INFO - No checkpoint file found, restarting from scratch
[1,1]<stdout>:2023-02-03 18:22:51,314 - INFO - train Step 0 metrics: Average/Non_Bkg_Accuracy: 0.154, Average/mIoU: 0.00148, loss: 3.79 (0.02 IOs / 0.047 (Step)(s))
[1,0]<stdout>:2023-02-03 18:22:52,452 - INFO - train Step 0 metrics: Average/Non_Bkg_Accuracy: 0.217, Average/mIoU: 0.00258, loss: 3.79 (0.018 IOs / 0.054 (Step)(s))
[1,1]<stdout>:2023-02-03 18:23:04,551 - INFO - train Step 1 metrics: Average/Non_Bkg_Accuracy: 0.187, Average/mIoU: 0.00254, loss: 3.86 (0.15 Img/s / 0.014 IOs / 0.039 (Step)(s))
[1,0]<stdout>:2023-02-03 18:23:06,185 - INFO - train Step 1 metrics: Average/Non_Bkg_Accuracy: 0.201, Average/mIoU: 0.00367, loss: 3.89 (0.15 Img/s / 0.02 IOs / 0.05 (Step)(s))
[1,1]<stdout>:2023-02-03 18:23:17,644 - INFO - train Step 2 metrics: Average/Non_Bkg_Accuracy: 0.177, Average/mIoU: 0.0012, loss: 3.76 (0.15 Img/s / 0.015 IOs / 0.044 (Step)(s))
[1,0]<stdout>:2023-02-03 18:23:19,911 - INFO - train Step 2 metrics: Average/Non_Bkg_Accuracy: 0.11, Average/mIoU: 0.000593, loss: 3.88 (0.15 Img/s / 0.016 IOs / 0.047 (Step)(s))
[1,1]<stdout>:2023-02-03 18:23:32,707 - INFO - train Step 3 metrics: Average/Non_Bkg_Accuracy: 0.165, Average/mIoU: 0.00136, loss: 3.86 (0.13 Img/s / 0.013 IOs / 0.067 (Step)(s))
[1,0]<stdout>:2023-02-03 18:23:33,349 - INFO - train Step 3 metrics: Average/Non_Bkg_Accuracy: 0.158, Average/mIoU: 0.0027, loss: 3.9 (0.15 Img/s / 0.018 IOs / 0.047 (Step)(s))
[1,1]<stdout>:2023-02-03 18:23:46,085 - INFO - train Step 4 metrics: Average/Non_Bkg_Accuracy: 0.156, Average/mIoU: 0.00207, loss: 3.81 (0.15 Img/s / 0.027 IOs / 0.045 (Step)(s))
[1,0]<stdout>:2023-02-03 18:23:46,375 - INFO - train Step 4 metrics: Average/Non_Bkg_Accuracy: 0.146, Average/mIoU: 0.00195, loss: 3.96 (0.15 Img/s / 0.018 IOs / 0.043 (Step)(s))
[1,1]<stdout>:2023-02-03 18:23:58,942 - INFO - train Step 5 metrics: Average/Non_Bkg_Accuracy: 0.16, Average/mIoU: 0.0026, loss: 3.97 (0.16 Img/s / 0.017 IOs / 0.045 (Step)(s))
[1,0]<stdout>:2023-02-03 18:23:59,390 - INFO - train Step 5 metrics: Average/Non_Bkg_Accuracy: 0.179, Average/mIoU: 0.00333, loss: 3.81 (0.15 Img/s / 0.015 IOs / 0.043 (Step)(s))
[1,1]<stdout>:2023-02-03 18:24:11,947 - INFO - train Step 6 metrics: Average/Non_Bkg_Accuracy: 0.156, Average/mIoU: 0.00192, loss: 3.85 (0.15 Img/s / 0.017 IOs / 0.041 (Step)(s))
[1,0]<stdout>:2023-02-03 18:24:12,394 - INFO - train Step 6 metrics: Average/Non_Bkg_Accuracy: 0.111, Average/mIoU: 0.00166, loss: 3.97 (0.15 Img/s / 0.018 IOs / 0.049 (Step)(s))
[1,1]<stdout>:2023-02-03 18:24:24,911 - INFO - train Step 7 metrics: Average/Non_Bkg_Accuracy: 0.221, Average/mIoU: 0.000799, loss: 3.65 (0.15 Img/s / 0.013 IOs / 0.04 (Step)(s))
[1,0]<stdout>:2023-02-03 18:24:25,284 - INFO - train Step 7 metrics: Average/Non_Bkg_Accuracy: 0.183, Average/mIoU: 0.00208, loss: 3.81 (0.16 Img/s / 0.014 IOs / 0.046 (Step)(s))
[1,1]<stdout>:2023-02-03 18:24:37,820 - INFO - train Step 8 metrics: Average/Non_Bkg_Accuracy: 0.168, Average/mIoU: 0.00177, loss: 3.88 (0.15 Img/s / 0.015 IOs / 0.045 (Step)(s))
[1,0]<stdout>:2023-02-03 18:24:38,210 - INFO - train Step 8 metrics: Average/Non_Bkg_Accuracy: 0.16, Average/mIoU: 0.00276, loss: 3.9 (0.15 Img/s / 0.015 IOs / 0.039 (Step)(s))
[1,1]<stdout>:2023-02-03 18:24:51,030 - INFO - train Step 9 metrics: Average/Non_Bkg_Accuracy: 0.2, Average/mIoU: 0.00323, loss: 3.79 (0.15 Img/s / 0.013 IOs / 0.038 (Step)(s))
[1,1]<stdout>:2023-02-03 18:24:51,049 - INFO - Saving run profile information.
[1,1]<stdout>:2023-02-03 18:24:51,066 - INFO - Total time to batch_process: 133.5980
[1,1]<stdout>:2023-02-03 18:24:51,066 - INFO - Total time to batch process except first iteration: 119.7481, throughput: 0.1503
[1,1]<stdout>:2023-02-03 18:24:51,066 - INFO - Total time to batch process except first two iterations: 106.5061, throughput: 0.1502
[1,0]<stdout>:2023-02-03 18:24:51,135 - INFO - train Step 9 metrics: Average/Non_Bkg_Accuracy: 0.13, Average/mIoU: 0.00097, loss: 3.8 (0.15 Img/s / 0.016 IOs / 0.045 (Step)(s))
[1,0]<stdout>:2023-02-03 18:24:51,157 - INFO - Saving run profile information.
[1,0]<stdout>:2023-02-03 18:24:51,169 - INFO - Total time to batch_process: 133.7065
[1,0]<stdout>:2023-02-03 18:24:51,169 - INFO - Total time to batch process except first iteration: 118.7121, throughput: 0.1516
[1,0]<stdout>:2023-02-03 18:24:51,169 - INFO - Total time to batch process except first two iterations: 104.9743, throughput: 0.1524
18:24:51.815 774863 POPRUN [I] mpirun (PID 775437) terminated with exit code 0
